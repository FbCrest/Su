// geminiService.js - Service for interacting with the Gemini API
import { getApiKey, getSelectedModel } from '../utils/apiKeyManager';
import { showNotification } from '../utils/notifications';
import { getSegmentDuration } from '../utils/timeUtils';
import { getYouTubeVideoId } from '../utils/videoUtils';
import { getSegmentedSubtitles } from '../utils/subtitleUtils';
import { getLanguageCode } from '../utils/languageUtils';
import { t } from 'i18next';

// Default prompt for transcription
export const DEFAULT_TRANSCRIPTION_PROMPT = `Transcribe the speech in this video accurately. Format the output as a transcript with BOTH start AND end timestamps for each line in the format [START_TIME - END_TIME] where times are in the format MMmSSsNNNms (minutes, seconds, milliseconds). For example: [0m30s000ms - 0m35s500ms] or [1m45s200ms - 1m50s000ms]. Each subtitle entry should be 1-2 sentences maximum. Return ONLY the transcript and no other text.`;

// Default prompt for consolidation
export const DEFAULT_CONSOLIDATION_PROMPT = `Combine these subtitle segments into a coherent transcript. Maintain the exact timestamps from the original segments. Format each line as [START_TIME - END_TIME] followed by the text. Ensure proper sentence structure, correct any obvious transcription errors, and maintain the original meaning. Return only the formatted transcript with no additional text.`;

// Default prompt for summarization
export const DEFAULT_SUMMARIZATION_PROMPT = `Summarize the following transcript in a concise way that captures the main points and key information. Keep the summary to about 10% of the original length.`;

// Default prompt for translation
export const DEFAULT_TRANSLATION_PROMPT = `Translate the following subtitles to {{targetLanguage}}. Maintain the exact same meaning and tone as the original. Return only the translated text for each subtitle, keeping the same format with one subtitle per line.`;

// Prompt presets for different use cases
export const PROMPT_PRESETS = [
    {
        id: 'general',
        title: 'General purpose',
        prompt: `Transcribe the speech in this ${'{contentType}'} accurately. Format the output as a transcript with BOTH start AND end timestamps for each line in the format [START_TIME - END_TIME] where times are in the format MMmSSsNNNms (minutes, seconds, milliseconds). For example: [0m30s000ms - 0m35s500ms] or [1m45s200ms - 1m50s000ms]. Each subtitle entry should be 1-2 sentences maximum. Return ONLY the transcript and no other text.`
    },
    {
        id: 'extract-text',
        title: 'Extract text',
        prompt: `Extract and transcribe ALL visible text in this ${'{contentType}'}, including signs, captions, titles, and any other text that appears on screen. Format the output as a transcript with BOTH start AND end timestamps for each text element in the format [START_TIME - END_TIME] where times are in the format MMmSSsNNNms (minutes, seconds, milliseconds). For example: [0m30s000ms - 0m35s500ms] or [1m45s200ms - 1m50s000ms]. Return ONLY the extracted text with timestamps and no other commentary.`
    },
    {
        id: 'focus-spoken-words',
        title: 'Focus on Spoken Words',
        prompt: `Transcribe ONLY the spoken words in this ${'{contentType}'} with high accuracy. Ignore background noises, music, or sound effects. Format the output as a transcript with BOTH start AND end timestamps for each line in the format [START_TIME - END_TIME] where times are in the format MMmSSsNNNms (minutes, seconds, milliseconds). For example: [0m30s000ms - 0m35s500ms] or [1m45s200ms - 1m50s000ms]. Each subtitle entry should be 1-2 sentences maximum. Return ONLY the transcript and no other text.`
    },
    {
        id: 'focus-lyrics',
        title: 'Focus on Lyrics',
        prompt: `Transcribe ONLY the song lyrics in this ${'{contentType}'} with high accuracy. Format the output as a transcript with BOTH start AND end timestamps for each line in the format [START_TIME - END_TIME] where times are in the format MMmSSsNNNms (minutes, seconds, milliseconds). For example: [0m30s000ms - 0m35s500ms] or [1m45s200ms - 1m50s000ms]. Each subtitle entry should be 1-2 lines of lyrics maximum. Return ONLY the lyrics transcript and no other text.`
    },
    {
        id: 'describe-video',
        title: 'Describe video',
        prompt: `Describe the significant visual events, actions, and scene changes occurring in this ${'{contentType}'} in chronological order. Focus solely on what is visually happening on screen. Format the output as a descriptive log. Each line MUST strictly follow the format: [MMmSSsNNNms - MMmSSsNNNms] Visual description (1-2 sentences max). For example: [0m30s000ms - 0m35s500ms] A person walks across the screen. Return ONLY the formatted descriptions with their timestamps. Do not include any audio transcription, headers, or other commentary.`
    },
    {
        id: 'translate-vietnamese',
        title: 'Translate directly',
        prompt: `Identify the spoken language(s) in this ${'{contentType}'} and translate the speech directly into Vietnamese. If multiple languages are spoken, translate all spoken segments into Vietnamese. Format the output as a sequential transcript of the translation. Each line MUST strictly follow the format: [MMmSSsNNNms - MMmSSsNNNms] translated text (1-2 translated sentences max). Return ONLY the formatted translation lines with timestamps. Do not include the original language transcription, headers, or any other text.`
    },
    {
        id: 'chaptering',
        title: 'Chaptering',
        prompt: `Analyze this ${'{contentType}'} and identify distinct chapters or segments based on topic changes. For each chapter, provide:
1. A concise, descriptive title (max 5-7 words)
2. The exact timestamp where the chapter begins in the format [HH:MM:SS]
3. A brief 1-2 sentence summary of what the chapter covers

Format your response as follows:
[00:00:00] Introduction
Brief description of the opening segment.

[MM:SS:FF] Chapter Title
Brief description of what this chapter covers.

Continue this pattern for all identified chapters. Be precise with timestamps and make sure chapter titles are descriptive but concise. Focus on major topic changes rather than minor shifts in conversation.`
    },
    {
        id: 'diarize-speakers',
        title: 'Identify Speakers',
        prompt: `Identify and label different speakers in this ${'{contentType}'}. For each spoken segment, indicate who is speaking by assigning speaker labels (Speaker 1, Speaker 2, etc.) or names if they can be identified from context. Format the output as a transcript with BOTH start AND end timestamps for each line in the format [START_TIME - END_TIME] where times are in the format MMmSSsNNNms (minutes, seconds, milliseconds). For example: [0m30s000ms - 0m35s500ms] Speaker 1: This is what I said. Return ONLY the transcript with speaker labels and no other text.`
    }
];

/**
 * Process a video or audio file with Gemini
 * @param {Object} options - Processing options
 * @param {string} options.mediaType - Type of media ('video' or 'audio')
 * @param {string} options.mediaSource - Source of the media (URL, file path, etc.)
 * @param {string} options.prompt - Prompt for Gemini
 * @param {string} options.model - Gemini model to use
 * @param {Function} options.onProgress - Progress callback
 * @param {Function} options.onSegmentComplete - Callback when a segment is complete
 * @param {Function} options.onError - Error callback
 * @param {Function} options.onComplete - Completion callback
 * @param {number} options.segmentDuration - Duration of each segment in seconds
 * @param {boolean} options.isRetry - Whether this is a retry attempt
 * @param {string} options.segmentId - ID of the segment to retry
 * @param {string} options.videoId - ID of the video (for YouTube videos)
 * @param {string} options.videoTitle - Title of the video
 * @param {string} options.videoThumbnail - Thumbnail URL of the video
 * @param {string} options.videoDuration - Duration of the video in seconds
 * @returns {Promise<Object>} - Processing result
 */
export const processMediaWithGemini = async (options) => {
    const {
        mediaType = 'video',
        mediaSource,
        prompt = DEFAULT_TRANSCRIPTION_PROMPT,
        model = getSelectedModel(),
        onProgress = () => { },
        onSegmentComplete = () => { },
        onError = () => { },
        onComplete = () => { },
        segmentDuration = 1200, // 20 minutes default
        isRetry = false,
        segmentId = null,
        videoId = null,
        videoTitle = null,
        videoThumbnail = null,
        videoDuration = null
    } = options;

    // Validate required parameters
    if (!mediaSource) {
        const error = new Error('Media source is required');
        onError(error);
        return { success: false, error };
    }

    // Create a unique ID for this request
    const requestId = `request_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;

    try {
        // Log request data for debugging (without the actual base64 data to keep logs clean)
        console.log(`[Gemini] Processing ${mediaType} with model: ${model}`);
        console.log(`[Gemini] Request ID: ${requestId}`);
        console.log(`[Gemini] Segment duration: ${segmentDuration} seconds`);
        console.log(`[Gemini] Is retry: ${isRetry}`);
        console.log(`[Gemini] Segment ID: ${segmentId}`);

        // Get API key from localStorage
        const apiKey = getApiKey();
        if (!apiKey) {
            const error = new Error('Gemini API key is not set. Please set it in the settings.');
            onError(error);
            return { success: false, error };
        }

        // For YouTube videos, extract the video ID
        let extractedVideoId = videoId;
        if (!extractedVideoId && mediaSource.includes('youtube.com') || mediaSource.includes('youtu.be')) {
            extractedVideoId = getYouTubeVideoId(mediaSource);
        }

        // Process the media
        if (isRetry && segmentId) {
            // Retry a specific segment
            const result = await processSegment({
                mediaType,
                mediaSource,
                prompt,
                model,
                apiKey,
                segmentId,
                onProgress,
                requestId,
                videoId: extractedVideoId,
                videoTitle,
                videoThumbnail,
                videoDuration
            });

            onSegmentComplete(result);
            onComplete([result]);
            return { success: true, results: [result] };
        } else {
            // Process the entire media by segments
            const segments = await getSegmentedSubtitles(mediaSource, segmentDuration);
            const totalSegments = segments.length;
            const results = [];

            console.log(`[Gemini] Total segments: ${totalSegments}`);

            // Process each segment
            for (let i = 0; i < segments.length; i++) {
                const segment = segments[i];
                const segmentProgress = (i / totalSegments) * 100;
                onProgress(segmentProgress, i, totalSegments);

                try {
                    const result = await processSegment({
                        mediaType,
                        mediaSource: segment.url,
                        prompt,
                        model,
                        apiKey,
                        segmentId: segment.id,
                        onProgress: (progress) => {
                            // Calculate overall progress
                            const overallProgress = segmentProgress + (progress / totalSegments);
                            onProgress(overallProgress, i, totalSegments, progress);
                        },
                        requestId,
                        startTime: segment.startTime,
                        endTime: segment.endTime,
                        videoId: extractedVideoId,
                        videoTitle,
                        videoThumbnail,
                        videoDuration
                    });

                    results.push(result);
                    onSegmentComplete(result);
                } catch (error) {
                    console.error(`[Gemini] Error processing segment ${i}:`, error);

                    // Create an error result for this segment
                    const errorResult = {
                        id: segment.id,
                        success: false,
                        error: error.message,
                        startTime: segment.startTime,
                        endTime: segment.endTime,
                        subtitles: [],
                        rawResponse: null
                    };

                    results.push(errorResult);
                    onSegmentComplete(errorResult);

                    // Show notification for segment error
                    showNotification(
                        'error',
                        t('errors.segmentProcessingError', 'Error processing segment {{segmentNumber}}', { segmentNumber: i + 1 }),
                        error.message
                    );
                }
            }

            onComplete(results);
            return { success: true, results };
        }
    } catch (error) {
        console.error('[Gemini] Error processing media:', error);
        onError(error);
        return { success: false, error };
    }
};

/**
 * Process a single segment with Gemini
 * @param {Object} options - Processing options
 * @returns {Promise<Object>} - Processing result
 */
const processSegment = async (options) => {
    const {
        mediaType,
        mediaSource,
        prompt,
        model,
        apiKey,
        segmentId,
        onProgress = () => { },
        requestId,
        startTime = 0,
        endTime = 0,
        videoId = null,
        videoTitle = null,
        videoThumbnail = null,
        videoDuration = null
    } = options;

    // Create a unique ID for this segment if not provided
    const id = segmentId || `segment_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;

    try {
        // Prepare the request data
        const contentType = mediaType === 'video' ? 'video' : 'audio';
        const formattedPrompt = prompt.replace('{contentType}', contentType);

        // Log segment processing
        console.log(`[Gemini] Processing segment ${id} (${startTime}s - ${endTime}s)`);
        console.log(`[Gemini] Media source: ${mediaSource}`);
        console.log(`[Gemini] Model: ${model}`);

        // Simulate Gemini API call (replace with actual implementation)
        // This is a placeholder for the actual API call
        const response = await simulateGeminiApiCall(mediaSource, formattedPrompt, model, apiKey, onProgress);

        // Parse the response to extract subtitles
        const subtitles = parseSubtitlesFromResponse(response, startTime);

        // Return the result
        return {
            id,
            success: true,
            startTime,
            endTime,
            subtitles,
            rawResponse: response,
            videoId,
            videoTitle,
            videoThumbnail,
            videoDuration
        };
    } catch (error) {
        console.error(`[Gemini] Error processing segment ${id}:`, error);
        throw error;
    }
};

/**
 * Translate subtitles using Gemini
 * @param {Object} options - Translation options
 * @param {Array} options.subtitles - Subtitles to translate
 * @param {string} options.targetLanguage - Target language
 * @param {string} options.prompt - Custom prompt for translation
 * @param {string} options.model - Gemini model to use
 * @param {Function} options.onProgress - Progress callback
 * @param {Function} options.onComplete - Completion callback
 * @param {Function} options.onError - Error callback
 * @returns {Promise<Object>} - Translation result
 */
export const translateSubtitlesWithGemini = async (options) => {
    const {
        subtitles,
        targetLanguage,
        prompt = DEFAULT_TRANSLATION_PROMPT,
        model = getSelectedModel(),
        onProgress = () => { },
        onComplete = () => { },
        onError = () => { }
    } = options;

    // Validate required parameters
    if (!subtitles || !subtitles.length) {
        const error = new Error('Subtitles are required for translation');
        onError(error);
        return { success: false, error };
    }

    if (!targetLanguage) {
        const error = new Error('Target language is required for translation');
        onError(error);
        return { success: false, error };
    }

    // Create a unique ID for this request
    const requestId = `translation_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;

    try {
        // Get API key from localStorage
        const apiKey = getApiKey();
        if (!apiKey) {
            const error = new Error('Gemini API key is not set. Please set it in the settings.');
            onError(error);
            return { success: false, error };
        }

        // Format the prompt with the target language
        const formattedPrompt = prompt.replace('{{targetLanguage}}', targetLanguage);

        // Prepare the subtitles text
        const subtitlesText = subtitles.map(sub => `${sub.text}`).join('\n\n');

        // Log translation request
        console.log(`[Gemini] Translating subtitles to ${targetLanguage}`);
        console.log(`[Gemini] Request ID: ${requestId}`);
        console.log(`[Gemini] Model: ${model}`);
        console.log(`[Gemini] Number of subtitles: ${subtitles.length}`);

        // Simulate Gemini API call for translation
        const response = await simulateGeminiTextCall(subtitlesText, formattedPrompt, model, apiKey, onProgress);

        // Parse the response to extract translated subtitles
        // Import the parseTranslatedSubtitles function from subtitleParser
        const { parseTranslatedSubtitles } = await import('../utils/subtitleParser');

        // Log the full response for debugging
        console.log(`[Gemini] Translation response length: ${response.length} characters`);

        // Use the proper parsing function to handle the translated subtitles
        // This will correctly parse the SRT format and maintain all subtitle information
        let translatedSubtitles = await parseTranslatedSubtitles(response);

        // If parsing failed or returned empty results, fall back to a simpler approach
        if (!translatedSubtitles || translatedSubtitles.length === 0) {
            console.log('[Gemini] Falling back to simple line-by-line translation parsing');
            const translatedLines = response.split('\n').filter(line => line.trim());

            translatedSubtitles = subtitles.map((sub, index) => {
                // If we have a translated line for this subtitle, use it
                // Otherwise keep the original text
                const translatedText = index < translatedLines.length ? translatedLines[index] : sub.text;
                return {
                    ...sub,
                    text: translatedText,
                    language: getLanguageCode(targetLanguage)
                };
            });
        } else {
            // Add language code to all translated subtitles
            translatedSubtitles = translatedSubtitles.map(sub => ({
                ...sub,
                language: getLanguageCode(targetLanguage)
            }));
        }

        // Complete the translation
        onComplete(translatedSubtitles);
        return { success: true, translatedSubtitles };
    } catch (error) {
        console.error('[Gemini] Error translating subtitles:', error);
        onError(error);
        return { success: false, error };
    }
};

/**
 * Process a document with Gemini
 * @param {Object} options - Processing options
 * @param {string} options.text - Text to process
 * @param {string} options.prompt - Prompt for Gemini
 * @param {string} options.model - Gemini model to use
 * @param {Function} options.onProgress - Progress callback
 * @param {Function} options.onComplete - Completion callback
 * @param {Function} options.onError - Error callback
 * @returns {Promise<Object>} - Processing result
 */
export const processDocumentWithGemini = async (options) => {
    const {
        text,
        prompt,
        model = getSelectedModel(),
        onProgress = () => { },
        onComplete = () => { },
        onError = () => { }
    } = options;

    // Validate required parameters
    if (!text) {
        const error = new Error('Text is required for document processing');
        onError(error);
        return { success: false, error };
    }

    if (!prompt) {
        const error = new Error('Prompt is required for document processing');
        onError(error);
        return { success: false, error };
    }

    // Create a unique ID for this request
    const requestId = `document_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;

    try {
        // Get API key from localStorage
        const apiKey = getApiKey();
        if (!apiKey) {
            const error = new Error('Gemini API key is not set. Please set it in the settings.');
            onError(error);
            return { success: false, error };
        }

        // Log document processing request
        console.log(`[Gemini] Processing document`);
        console.log(`[Gemini] Request ID: ${requestId}`);
        console.log(`[Gemini] Model: ${model}`);
        console.log(`[Gemini] Text length: ${text.length}`);

        // Simulate Gemini API call for document processing
        const response = await simulateGeminiTextCall(text, prompt, model, apiKey, onProgress);

        // Complete the processing
        onComplete(response);
        return { success: true, response };
    } catch (error) {
        console.error('[Gemini] Error processing document:', error);
        onError(error);
        return { success: false, error };
    }
};

/**
 * Summarize subtitles using Gemini
 * @param {Object} options - Summarization options
 * @param {Array} options.subtitles - Subtitles to summarize
 * @param {string} options.prompt - Custom prompt for summarization
 * @param {string} options.model - Gemini model to use
 * @param {Function} options.onProgress - Progress callback
 * @param {Function} options.onComplete - Completion callback
 * @param {Function} options.onError - Error callback
 * @returns {Promise<Object>} - Summarization result
 */
export const summarizeSubtitlesWithGemini = async (options) => {
    const {
        subtitles,
        prompt = DEFAULT_SUMMARIZATION_PROMPT,
        model = getSelectedModel(),
        onProgress = () => { },
        onComplete = () => { },
        onError = () => { }
    } = options;

    // Validate required parameters
    if (!subtitles || !subtitles.length) {
        const error = new Error('Subtitles are required for summarization');
        onError(error);
        return { success: false, error };
    }

    // Create a unique ID for this request
    const requestId = `summary_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;

    try {
        // Get API key from localStorage
        const apiKey = getApiKey();
        if (!apiKey) {
            const error = new Error('Gemini API key is not set. Please set it in the settings.');
            onError(error);
            return { success: false, error };
        }

        // Prepare the subtitles text
        const subtitlesText = subtitles.map(sub => `${sub.text}`).join('\n\n');

        // Log summarization request
        console.log(`[Gemini] Summarizing subtitles`);
        console.log(`[Gemini] Request ID: ${requestId}`);
        console.log(`[Gemini] Model: ${model}`);
        console.log(`[Gemini] Number of subtitles: ${subtitles.length}`);

        // Simulate Gemini API call for summarization
        const response = await simulateGeminiTextCall(subtitlesText, prompt, model, apiKey, onProgress);

        // Complete the summarization
        onComplete(response);
        return { success: true, summary: response };
    } catch (error) {
        console.error('[Gemini] Error summarizing subtitles:', error);
        onError(error);
        return { success: false, error };
    }
};

/**
 * Consolidate subtitles using Gemini
 * @param {Object} options - Consolidation options
 * @param {Array} options.subtitles - Subtitles to consolidate
 * @param {string} options.prompt - Custom prompt for consolidation
 * @param {string} options.model - Gemini model to use
 * @param {Function} options.onProgress - Progress callback
 * @param {Function} options.onComplete - Completion callback
 * @param {Function} options.onError - Error callback
 * @returns {Promise<Object>} - Consolidation result
 */
export const consolidateSubtitlesWithGemini = async (options) => {
    const {
        subtitles,
        prompt = DEFAULT_CONSOLIDATION_PROMPT,
        model = getSelectedModel(),
        onProgress = () => { },
        onComplete = () => { },
        onError = () => { }
    } = options;

    // Implementation similar to summarizeSubtitlesWithGemini
    // ...

    return { success: true, consolidatedSubtitles: [] };
};

// Placeholder for the actual Gemini API call implementation
const simulateGeminiApiCall = async (mediaSource, prompt, model, apiKey, onProgress) => {
    // This is a placeholder for the actual API call
    // In a real implementation, this would call the Gemini API with the media file

    // Simulate progress
    for (let i = 0; i <= 100; i += 10) {
        onProgress(i);
        await new Promise(resolve => setTimeout(resolve, 100));
    }

    // Return a simulated response
    return `[0m00s000ms - 0m05s000ms] This is a simulated subtitle response.
[0m05s000ms - 0m10s000ms] It would be replaced with actual Gemini API results.
[0m10s000ms - 0m15s000ms] The real implementation would process the media file.
[0m15s000ms - 0m20s000ms] And return properly formatted subtitles.`;
};

// Implementation of the Gemini text API call
const simulateGeminiTextCall = async (text, prompt, model, apiKey, onProgress) => {
    try {
        // Create a unique request ID
        const requestId = `text_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;

        // Log the request details
        console.log(`[Gemini] Text API request ${requestId}`);
        console.log(`[Gemini] Model: ${model}`);
        console.log(`[Gemini] Text length: ${text.length} characters`);
        console.log(`[Gemini] Prompt length: ${prompt.length} characters`);

        // Report initial progress
        onProgress(10);

        // Make the actual API call to Gemini
        const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`;

        // Report progress before fetch
        onProgress(30);

        const response = await fetch(apiUrl, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                contents: [
                    {
                        role: "user",
                        parts: [
                            { text: prompt }
                        ]
                    }
                ],
                generationConfig: {
                    temperature: 0.2,
                    topK: 32,
                    topP: 0.95,
                    maxOutputTokens: 65536, // Increased to maximum allowed value (65536 per Gemini documentation)
                },
            })
        });

        // Report progress after fetch
        onProgress(70);

        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(`Gemini API error: ${errorData.error?.message || response.statusText}`);
        }

        const data = await response.json();
        const resultText = data.candidates[0]?.content?.parts[0]?.text;

        if (!resultText) {
            throw new Error('No text returned from Gemini API');
        }

        // Report completion
        onProgress(100);

        // Log success
        console.log(`[Gemini] Text API request ${requestId} completed successfully`);
        console.log(`[Gemini] Response length: ${resultText.length} characters`);

        return resultText;
    } catch (error) {
        console.error('[Gemini] Error in text API call:', error);
        throw error;
    }
};

// Helper function to parse subtitles from Gemini response
const parseSubtitlesFromResponse = (response, startTimeOffset = 0) => {
    if (!response) return [];

    const lines = response.split('\n').filter(line => line.trim());
    const subtitles = [];

    for (const line of lines) {
        // Match timestamp pattern [MMmSSsNNNms - MMmSSsNNNms]
        const timestampMatch = line.match(/\[(\d+)m(\d+)s(\d+)ms - (\d+)m(\d+)s(\d+)ms\]/);

        if (timestampMatch) {
            // Extract start and end times
            const startMin = parseInt(timestampMatch[1]);
            const startSec = parseInt(timestampMatch[2]);
            const startMs = parseInt(timestampMatch[3]);
            const endMin = parseInt(timestampMatch[4]);
            const endSec = parseInt(timestampMatch[5]);
            const endMs = parseInt(timestampMatch[6]);

            // Convert to seconds
            const startTime = startMin * 60 + startSec + startMs / 1000 + startTimeOffset;
            const endTime = endMin * 60 + endSec + endMs / 1000 + startTimeOffset;

            // Extract the text content (everything after the timestamp)
            const text = line.substring(line.indexOf(']') + 1).trim();

            // Add to subtitles array
            subtitles.push({
                id: `subtitle_${subtitles.length + 1}`,
                start: startTime,
                end: endTime,
                text,
                startTime: formatTimeString(startTime),
                endTime: formatTimeString(endTime)
            });
        }
    }

    return subtitles;
};

// Helper function to format time in HH:MM:SS.mmm format
const formatTimeString = (timeInSeconds) => {
    const hours = Math.floor(timeInSeconds / 3600);
    const minutes = Math.floor((timeInSeconds % 3600) / 60);
    const seconds = Math.floor(timeInSeconds % 60);
    const milliseconds = Math.floor((timeInSeconds % 1) * 1000);

    return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}.${milliseconds.toString().padStart(3, '0')}`;
};
